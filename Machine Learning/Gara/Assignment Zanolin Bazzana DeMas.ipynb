{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbpQdieHhvbd"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "## Dataset\n",
        "[Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult)\n",
        "\n",
        "Training set: 32'561 records\n",
        "\n",
        "Test set: 16'280 records\n",
        "\n",
        "### Goal\n",
        "\n",
        "Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset.\n",
        "\n",
        "### Features (14)\n",
        "\n",
        "Note: features contain missing data!\n",
        "\n",
        "| Name           | Type        |\n",
        "|----------------|-------------|\n",
        "| age            | continuous  |\n",
        "| workclass      | categorical |\n",
        "| fnlwgt         | continuous  |\n",
        "| education      | categorical |\n",
        "| education-num  | continuous  |\n",
        "| marital-status | categorical |\n",
        "| occupation     | categorical |\n",
        "| relationship   | categorical |\n",
        "| race           | categorical |\n",
        "| sex            | categorical |\n",
        "| capital-gain   | continuous  |\n",
        "| capital-loss   | continuous  |\n",
        "| hours-per-week | continuous  |\n",
        "| native-country | categorical |\n",
        "\n",
        "### Target (1)\n",
        "\n",
        "| Name           | Type                                |\n",
        "|----------------|-------------------------------------|\n",
        "| income         | categorical<br>(\"<=50K\" or \">50K\")  |\n",
        "\n",
        "### Evaluation Metrics\n",
        "\n",
        "Accuracy and F1 Score\n",
        "\n",
        "# Deadline\n",
        "\n",
        "Sunday 29/05/2022 at 23:59\n",
        "\n",
        "To submit:\n",
        "- a zip file containing the code for the model (briefly commented) or a link to a Colab notebook (make sure the link is public)\n",
        "- a csv/txt with one line for each test sample, containing the predictions\n",
        "- note that we should be able to run the code and obtain the same predictions present in the csv/txt file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhOvmw_Ck1Dm"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx6Wv0XQkvRZ"
      },
      "outputs": [],
      "source": [
        "# Data loading\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
        "\n",
        "column_names=[\n",
        "    \"age\",\n",
        "    \"workclass\",\n",
        "    \"fnlwgt\",\n",
        "    \"education\",\n",
        "    \"education-num\",\n",
        "    \"marital-status\",\n",
        "    \"occupation\",\n",
        "    \"relationship\",\n",
        "    \"race\",\n",
        "    \"sex\",\n",
        "    \"capital-gain\",\n",
        "    \"capital-loss\",\n",
        "    \"hours-per-week\",\n",
        "    \"native-country\",\n",
        "    \"income\"\n",
        "]\n",
        "\n",
        "df_train = pd.read_csv(\"adult.data\",index_col=False, names=column_names)\n",
        "df_test = pd.read_csv(\"adult.test\", index_col=False, skiprows=1, names=column_names)\n",
        "real_predictions = df_test.income\n",
        "real_predictions.to_csv(\"predictions.test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6a1_N0NyWY9"
      },
      "source": [
        "#Codifica delle categorie\n",
        "\n",
        "Le categorie (attributi che sono in un range di valori predefiniti) vanno codificate con la tecnica One-Hot encode, mentre i campi numerici vanno lasciati inalterati."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "crGNyCWhETom"
      },
      "outputs": [],
      "source": [
        "from os import pathconf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Elimina tutte le tuple contenenti valori anomali\n",
        "def refine(data):\n",
        "  out = []\n",
        "  cols = len(data.columns)\n",
        "  for i in range(len(data)):\n",
        "    arr = []\n",
        "    for l in range(cols):\n",
        "      v = data.iat[i,l]\n",
        "      if v == ' ?':\n",
        "        arr = None\n",
        "        break\n",
        "      else:\n",
        "        arr.append(v)\n",
        "    if arr != None:\n",
        "      out.append(arr)\n",
        "    \n",
        "  dataFrame = pd.DataFrame(out)\n",
        "  dataFrame.columns = data.columns\n",
        "  return dataFrame\n",
        "\n",
        "\n",
        "# codifica le feature categoriche (con > 2 categorie) in una rappresentazione one hot.\n",
        "# codifica le feature categoriche (con = 2 categorie) in una rappresentazione label.\n",
        "def encode_data(data, keep_unknown=False):\n",
        "\n",
        "  categories = {\n",
        "           'workclass': [['Private'], ['Self-emp-not-inc'], ['Self-emp-inc'], ['Federal-gov'], ['Local-gov'], ['State-gov'], ['Without-pay'], ['Never-worked']],\n",
        "           'education':[['Bachelors'], ['Some-college'],['11th'], ['HS-grad'], ['Prof-school'], ['Assoc-acdm'], ['Assoc-voc'], ['9th'], ['7th-8th'], ['12th'], ['Masters'], ['1st-4th'], ['10th'], ['Doctorate'], ['5th-6th'], ['Preschool']],\n",
        "           'marital-status':[['Married-civ-spouse'], ['Divorced'], ['Never-married'], ['Separated'], ['Widowed'], ['Married-spouse-absent'], ['Married-AF-spouse']],\n",
        "           'occupation':[['Tech-support'], ['Craft-repair'], ['Other-service'], ['Sales'], ['Exec-managerial'], ['Prof-specialty'], ['Handlers-cleaners'], ['Machine-op-inspct'], ['Adm-clerical'], ['Farming-fishing'], ['Transport-moving'], ['Priv-house-serv'], ['Protective-serv'], ['Armed-Forces']],\n",
        "           'relationship':[['Wife'], ['Own-child'], ['Husband'], ['Not-in-family'], ['Other-relative'], ['Unmarried']],\n",
        "           'race':[['White'], ['Asian-Pac-Islander'], ['Amer-Indian-Eskimo'], ['Other'], ['Black']],\n",
        "           'sex':[['Male'],['Female']],\n",
        "           'native-country':[['United-States'], ['Cambodia'], ['England'], ['Puerto-Rico'], ['Canada'], ['Germany'], ['Outlying-US(Guam-USVI-etc)'], ['India'], ['Japan'], ['Greece'], ['South'], ['China'], ['Cuba'], ['Iran'], ['Honduras'], ['Philippines'], ['Italy'], ['Poland'], ['Jamaica'], ['Vietnam'], ['Mexico'], ['Portugal'], ['Ireland'], ['France'], ['Dominican-Republic'], ['Laos'], ['Ecuador'], ['Taiwan'], ['Haiti'], ['Columbia'], ['Hungary'], ['Guatemala'], ['Nicaragua'], ['Scotland'], ['Thailand'], ['Yugoslavia'], ['El-Salvador'], ['Trinadad&Tobago'], ['Peru'], ['Hong'], ['Holand-Netherlands']]\n",
        "  }\n",
        "    \n",
        "  for col in categories:\n",
        "    enc = OneHotEncoder(handle_unknown='ignore', drop='if_binary')\n",
        "\n",
        "    enc.fit(categories[col]) # genera la rappresentazione one-hot della feature singola\n",
        "    enc_arr = pd.DataFrame(enc.transform([[x.strip()] for x in data[col]]).toarray()) # trasforma la colonna nella rappresentazione one-hot e genera un dataframe a partire da essa\n",
        "    enc_arr.columns = [col + str(i) for i in range(1, enc_arr.shape[1]+1)] # nuova label per la colonna\n",
        "\n",
        "    data = pd.concat([data.loc[:,:col].iloc[:,:-1], enc_arr, data.loc[:,col:].iloc[:,1:]], axis=1) # posiziona il dataframe con la rappresentazione one-hot nella posizione giusta all'interno del dataframe completo\n",
        "    # data.loc[:,:col].iloc[:,:-1] -> tutte le colonne precedenti a quella che abbiamo trasformato\n",
        "    # data.loc[:,col:].iloc[:,1:] -> tutte le colonne successive\n",
        "    # loc e iloc vengono usati per prendere uno slice del dataframe, escludendo la colonna che stiamo trasformando \n",
        "\n",
        "  return data #dataframe codificato in One-Hot\n",
        "\n",
        "# codifica il target (y)\n",
        "def encode_target(data):\n",
        "  return [0 if \"<=50K\" in x else 1 for x in data]\n",
        "\n",
        "# Estrae le features\n",
        "# data -> il dataset\n",
        "def extractX(data):\n",
        "  return data.iloc[:,:-1]\n",
        "\n",
        "\n",
        "# Estrare la label\n",
        "# data -> il dataset\n",
        "def extractY(data):\n",
        "  return data.iloc[:,-1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS5N-5fgzM0I"
      },
      "source": [
        "#Creazione dei Training e Test set\n",
        "Si creano dei dataset prendendo i dati in input e pulendo le tuple inutili."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hY-3U8KE8y0",
        "outputId": "46a5b786-23fe-4e1d-a749-aeb8114201e9"
      },
      "outputs": [],
      "source": [
        "# Raffina e salva il training set -> toglie ?\n",
        "trainset = refine(df_train)\n",
        "trainset.to_csv(\"Dataset.data\", header=False, index=False)\n",
        "\n",
        "# Raffina e salva il test set\n",
        "#testset = refine(df_test) --> se vuoi raffinare anche il testSet\n",
        "testset = df_test\n",
        "testset.to_csv(\"Dataset.test\", index=False)\n",
        "\n",
        "# Estrae i dati usati nell'addestramento del modello \n",
        "X_train = extractX(trainset)\n",
        "Y_train = extractY(trainset)\n",
        "\n",
        "X_train = encode_data(X_train)\n",
        "Y_train = encode_target(Y_train)\n",
        "\n",
        "\n",
        "# Estrae i dati usati nell'addestramento del modello \n",
        "X_test = extractX(testset)\n",
        "Y_test = extractY(testset)\n",
        "\n",
        "# Codifica il test set\n",
        "\n",
        "X_test = encode_data(X_test)\n",
        "Y_test = encode_target(Y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF7tDdG1PcTq"
      },
      "source": [
        "#Compute Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "LsEpsCPhPexG"
      },
      "outputs": [],
      "source": [
        "# Compute Metrics\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(path_real, path_predicted):\n",
        "\n",
        "    column_names=[\n",
        "        \"age\",\n",
        "        \"workclass\",\n",
        "        \"fnlwgt\",\n",
        "        \"education\",\n",
        "        \"education-num\",\n",
        "        \"marital-status\",\n",
        "        \"occupation\",\n",
        "        \"relationship\",\n",
        "        \"race\",\n",
        "        \"sex\",\n",
        "        \"capital-gain\",\n",
        "        \"capital-loss\",\n",
        "        \"hours-per-week\",\n",
        "        \"native-country\",\n",
        "        \"income\"\n",
        "    ]\n",
        "    \n",
        "    if \"test\" in path_real:\n",
        "        df_real = pd.read_csv(path_real, index_col=False, skiprows=1, names=column_names)\n",
        "    else:\n",
        "        df_real = pd.read_csv(path_real, index_col=False, names=column_names)\n",
        "    \n",
        "    df_pred = pd.read_csv(path_predicted)\n",
        "    y_real = df_real.income.tolist()    #si salva, a parte, l'ultima colonna come una lista\n",
        "    y_pred = df_pred.income.tolist()    #si salva, a parte, l'ultima colonna come una lista\n",
        "\n",
        "    y_real = [0 if \"<=50K\" in x else 1 for x in y_real]\n",
        "    y_pred = [0 if \"<=50K\" in x else 1 for x in y_pred]\n",
        "    \n",
        "    accuracy = accuracy_score(y_real, y_pred)\n",
        "    f1 = f1_score(y_real, y_pred)\n",
        "\n",
        "    print(f\"Metrics comparing '{path_real}' and '{path_predicted}'\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"F1: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hnj5ZNW0Ix_"
      },
      "source": [
        "#Rete Neurale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLc46twfNip0"
      },
      "source": [
        "##Creazione della rete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "Gkk3bpRA3ELH"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "#RETE NEURALE\n",
        "from tensorflow import keras\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import metrics\n",
        "import numpy\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(len(X_train.columns),))) #la dimensione di input va da il numero di feature (104) ad un massimo di +inf\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.0005),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[metrics.accuracy])\n",
        "\n",
        "#scalo i dati con MinMax\n",
        "mM_scaler = MinMaxScaler()\n",
        "X_train_mM_scaled = mM_scaler.fit_transform(X_train)\n",
        "X_test_mM_scaled = mM_scaler.transform(X_test)\n",
        "\n",
        "#creo il validation set (di 1000 elementi) e i restanti sono usati per il training set\n",
        "X_Val = X_train_mM_scaled[:1000]\n",
        "Y_Val = numpy.array(Y_train[:1000])\n",
        "\n",
        "X_Train_Parziale = X_train_mM_scaled[1000:]\n",
        "Y_Train_Parziale = numpy.array(Y_train[1000:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzzD4Duv0pmI"
      },
      "source": [
        "##Allenamento della rete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MueCFrs97O9x",
        "outputId": "213014f3-b9b0-47a3-cb7a-23c45bb02b5f"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_Train_Parziale,\n",
        "                    Y_Train_Parziale,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_Val, Y_Val))\n",
        "\n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9kyuZ92-W_z"
      },
      "source": [
        "##Visualizzo l'andamento dell'allenamento della rete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNKSuuAa_Wk9"
      },
      "source": [
        "Andamento della funzione di loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "lHJ4Pk-8-Z0h",
        "outputId": "66e13eb8-3086-4a54-c778-62cd9989994f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkV9F-Re_bsv"
      },
      "source": [
        "Andamento della accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "JhvoWd6c_gBF",
        "outputId": "4ddd8f8a-fe35-48b3-e419-0817950ba52a"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og2-7rAd1MKW"
      },
      "source": [
        "##Utilizzo la rete per predirre i dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "mQryTeJd1PTV"
      },
      "outputs": [],
      "source": [
        "Risultati_Predetti = model.predict(X_Test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDfLa3jALeT5"
      },
      "source": [
        "##Calcolo dei risultati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "KH_WtEPELjpu"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj_IW6XrzxOC"
      },
      "source": [
        "##Salvataggio dei risultati"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9SRJVm9KH1_"
      },
      "source": [
        "Funzione per riconvertire da one-hot a categoria l'ultima cella, infatti solo quella verrà considerata nel calcolo delle metriche."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "_ap3KreXKGty"
      },
      "outputs": [],
      "source": [
        "# Decodifica le predizioni\n",
        "def decode_predictions(predictions): #restituisce un dataframe\n",
        "  return pd.DataFrame([\"<=50K\" if x == 0 else \">50K\" for x in predictions], columns=['income'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "Fefm5bMPz09o"
      },
      "outputs": [],
      "source": [
        "decoded_prediction = decode_predictions(predictions)\n",
        "decoded_prediction.to_csv(\"neural.data\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXJhl2F9mCL3"
      },
      "source": [
        "## Compute Metrics della rete Neurale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhZSo_orhsGg",
        "outputId": "e7230b33-6d14-4160-cfca-93d6e02215ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics comparing 'Dataset.test' and 'neural.data'\n",
            "Accuracy: 0.7934402063755298\n",
            "F1: 0.2677988242978446\n"
          ]
        }
      ],
      "source": [
        "compute_metrics(\"Dataset.test\", \"neural.data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9aLsUeROJoa"
      },
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5EE3_BiRy40"
      },
      "source": [
        "Librerie da importare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "0XuFN8ZMQzhC"
      },
      "outputs": [],
      "source": [
        "# LogisticRegression normale\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#LogisticRegression con normalizzazione standard\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#LogisticRegression con normalizzazione minmax\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# LogisticRegression con cross-validation\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_ZzkzhAOtjt"
      },
      "source": [
        "##Logistic Regression base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HjACr_hRHkY"
      },
      "source": [
        "Allenamento del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eBUIVlkOynn",
        "outputId": "7d977eed-5bd2-4eed-b921-3fff3eabf3fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.2, max_iter=1000, solver='saga')"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fitting del modello\n",
        "model = LogisticRegression(solver='saga', C=0.2, max_iter=1000)\n",
        "model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yql24nXyP1Sk"
      },
      "source": [
        "Salvo le predizioni e computo le *metriche*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ3MS6VFPsek",
        "outputId": "e846340f-51ad-475c-d358-110cc5587a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics comparing 'Dataset.test' and 'test_pred'\n",
            "Accuracy: 0.7871181938911023\n",
            "F1: 0.34114262227702424\n"
          ]
        }
      ],
      "source": [
        "test_predictions = model.predict(X_test)\n",
        "decode_predictions(test_predictions).to_csv(\"test_pred\", index=False)\n",
        "\n",
        "compute_metrics('Dataset.test', 'test_pred')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OM2zQA4Qqi7"
      },
      "source": [
        "##Logistic Regression con Normalizzazione std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXuT0uH9RMhw"
      },
      "source": [
        "Allenamento del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeRz1G0jQvdq",
        "outputId": "e1599fab-cd44-42ea-8989-e70bfe7110ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, max_iter=500, solver='saga')"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "scaled_model = LogisticRegression(solver='saga', C=0.1, max_iter=500) # converge entro 500 iterazioni\n",
        "scaled_model.fit(X_train_scaled, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxEAxBCPRc-c"
      },
      "source": [
        "Salvo le predizioni e computo le *metriche*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xD7-R7fRWSU",
        "outputId": "a145ced3-7a70-41f5-f260-2c4345d593dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics comparing 'Dataset.test' and 'test_pred'\n",
            "Accuracy: 0.847675962815405\n",
            "F1: 0.660751257024549\n"
          ]
        }
      ],
      "source": [
        "test_predictions = scaled_model.predict(X_test_scaled)\n",
        "decode_predictions(test_predictions).to_csv(\"test_pred\", index=False)\n",
        "\n",
        "compute_metrics('Dataset.test', 'test_pred')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmBAVbPARqlT"
      },
      "source": [
        "##Logistic Regression con Normalizzazione MIN/MAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbfTrjXySBIM"
      },
      "source": [
        "Allenamento del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e49tMt4FRu3g",
        "outputId": "494c79d3-4dbd-41c2-eb18-29ad647550e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression(solver='saga')"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mM_scaler = MinMaxScaler()\n",
        "X_train_mM_scaled = mM_scaler.fit_transform(X_train)\n",
        "\n",
        "X_test_mM_scaled = mM_scaler.transform(X_test)\n",
        "\n",
        "scaled_model = LogisticRegression(solver='saga') \n",
        "scaled_model.fit(X_train_mM_scaled, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cupr52spSDb8"
      },
      "source": [
        "Salvo le predizioni e computo le *metriche*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dObSCqzKSKg8",
        "outputId": "8f13fa8a-c621-4713-b8af-672eae6d848d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics comparing 'Dataset.test' and 'test_pred'\n",
            "Accuracy: 0.8460159362549801\n",
            "F1: 0.6555770087628101\n"
          ]
        }
      ],
      "source": [
        "test_predictions = scaled_model.predict(X_test_mM_scaled)\n",
        "decode_predictions(test_predictions).to_csv(\"test_pred\", index=False)\n",
        "\n",
        "compute_metrics('Dataset.test', 'test_pred')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33DOK_7SSjcd"
      },
      "source": [
        "##Logistic Regression con Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvuJAbc9Sljk"
      },
      "source": [
        "Allenamento del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPBBaId1SpBb",
        "outputId": "0eb05bf0-9834-4f71-a57d-dd80cffa1014"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegressionCV(Cs=[0.01, 0.1, 0.5, 1, 10], cv=5, max_iter=500,\n",
              "                     solver='saga')"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mM_scaler = MinMaxScaler()\n",
        "X_train_mM_scaled = mM_scaler.fit_transform(X_train)\n",
        "\n",
        "X_test_mM_scaled = mM_scaler.transform(X_test)\n",
        "C = [0.01, 0.1, 0.5, 1, 10]\n",
        "#solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}\n",
        "cv_model = LogisticRegressionCV(cv=5, Cs=C, solver='saga', max_iter=500)\n",
        "cv_model.fit(X_train_mM_scaled, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3BT3bHGSzXu"
      },
      "source": [
        "Salvo le predizioni e computo le metriche"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHPI1oVOS2Ku",
        "outputId": "b92826b1-6fc6-430f-c409-8d8e6f50b8e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics comparing 'Dataset.test' and 'test_pred'\n",
            "Accuracy: 0.8525889073152755\n",
            "F1: 0.6570448699628465\n"
          ]
        }
      ],
      "source": [
        "test_predictions = cv_model.predict(X_test_mM_scaled)\n",
        "decode_predictions(test_predictions).to_csv(\"test_pred\", index=False)\n",
        "\n",
        "compute_metrics('Dataset.test', 'test_pred')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "LbpQdieHhvbd",
        "yhOvmw_Ck1Dm",
        "g6a1_N0NyWY9",
        "WF7tDdG1PcTq",
        "1Hnj5ZNW0Ix_",
        "_9aLsUeROJoa",
        "9_ZzkzhAOtjt",
        "5OM2zQA4Qqi7"
      ],
      "name": "Assignment.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
